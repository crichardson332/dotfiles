---
tags: [AVA Lab]
title: '2021-08-26'
created: '2021-08-26T18:32:54.468Z'
modified: '2021-08-30T16:04:30.781Z'
---

# 2021-08-26

### High-level inference and reasoning

- Can we add a model-predictive element to existing architectures to better train contexual understanding?
  - Cognitive models (Jianfeng Gao talk)
  - In the talk, he says neural model can produce human like language, but it's not grounded in reality or facts
  - Can we use a predictive loss to train a world model, while using a neural model to produce fluid language?
  - **Passive listening**: use predictive error function to traina  predictive element on top of pretrained models/encoders?
    - Also called imagination
    - Will the pretrained models 

- Can we abstract concepts to a higher level?
  - Similarity learning for learning high level concepts
  - Non-verbalizable knowledge

- Inductive bias
  - probabilistic graphical models?

- Can we use some sort of associative memory of past conversations?

---
- Composable knowledge at the level of skills (hierarchical reasoning and inference)
- Challenges
  - Allow the assistant go where the user wants to go
  - 

---
- Adam shirer? - decision making summary AI
- Meeting summaries
  - Challenge: annotated datasets of meetings are scarce 


